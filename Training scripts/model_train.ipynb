{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5voFlz4l078D"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2Config\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kFV3fXixeCG",
        "outputId": "5e361783-b3bd-4fb7-ec3e-8ed500c3488c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHK6wPR_xee0"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = '/content/drive/My Drive/ece1786_project/model_checkpoint_ForSC.pth'\n",
        "# load_path = '/content/drive/My Drive/ece1786_project/model_checkpoint_stemmed1.pth'\n",
        "# state = torch.load(load_path)['model_state_dict']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLQlQkSv078F"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGmm_eLn078F"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzVCI7bT078F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/lyrics_preprocessed_unique.csv\")\n",
        "\n",
        "\n",
        "# sampling fewer data\n",
        "df = df.groupby('Mood_encod').apply(lambda x: x.sample(n=120, random_state=42)).reset_index(drop=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "lyrics = df['lyrics'].values\n",
        "mood = df['Mood_encod'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DRoHd0S078G"
      },
      "outputs": [],
      "source": [
        "df['lyrics'].shape\n",
        "df['Mood_encod'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06lkaMj5078G"
      },
      "outputs": [],
      "source": [
        "lyrics1 = lyrics.tolist()\n",
        "len(lyrics1)\n",
        "mood1 = mood.tolist()\n",
        "len(mood1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8SScdH4078G"
      },
      "outputs": [],
      "source": [
        "# splitting of data and tokenization\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split\\\n",
        "    (lyrics1, mood1, test_size = 0.2, random_state = 42)\n",
        "\n",
        "max_token_number = 512\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_inputs1 = tokenizer(train_inputs, padding=True, truncation=True, max_length = max_token_number, return_tensors=\"pt\")\n",
        "val_inputs1 = tokenizer(val_inputs, padding=True, truncation=True, max_length = max_token_number, return_tensors=\"pt\")\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "train_data = TensorDataset(train_inputs1.input_ids, train_inputs1.attention_mask, train_labels)\n",
        "val_data = TensorDataset(val_inputs1.input_ids, val_inputs1.attention_mask, val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EYVTycG078G"
      },
      "outputs": [],
      "source": [
        "# GPT2Model + Custom Classification Head Model\n",
        "\n",
        "# class GPT2MoodClassifier(nn.Module):\n",
        "#     def __init__(self, gpt2_model, num_classes = 4):\n",
        "#         super(GPT2MoodClassifier, self).__init__()\n",
        "#         self.gpt2_model = gpt2_model\n",
        "#         self.classification_head = nn.Linear(gpt2_model.config.hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         outputs = self.gpt2_model(input_ids, attention_mask = attention_mask)\n",
        "#         last_hidden_state = outputs[0]\n",
        "#         cls_hidden_state = last_hidden_state[:,0,:]\n",
        "#         logits = self.classification_head(cls_hidden_state)\n",
        "#         return logits\n",
        "\n",
        "# gpt2_model = GPT2Model.from_pretrained(model_name)\n",
        "# num_classes = len(set(mood))\n",
        "# model = GPT2MoodClassifier(gpt2_model, num_classes = num_classes)\n",
        "# model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT2ForSequenceClassification Model\n",
        "class GPT2MoodClassifier(nn.Module):\n",
        "    def __init__(self, gpt2_model, num_classes=4):\n",
        "        super(GPT2MoodClassifier, self).__init__()\n",
        "        self.gpt2_for_classification = gpt2_model\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.gpt2_for_classification(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "num_classes = len(set(mood))\n",
        "config = GPT2Config.from_pretrained(model_name, num_labels=num_classes, pad_token_id=50256)\n",
        "gpt2_model = GPT2ForSequenceClassification.from_pretrained(model_name, config = config)\n",
        "model = GPT2MoodClassifier(gpt2_model, num_classes = num_classes)"
      ],
      "metadata": {
        "id": "HV0XHuDVQokT",
        "outputId": "341c28df-39b6-4e4d-db10-9a1dab6698e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT2ForSequenceClassification Model with Regularization\n",
        "\n",
        "# class GPT2MoodClassifier(nn.Module):\n",
        "#     def __init__(self, gpt2_model, num_classes=4):\n",
        "#         super(GPT2MoodClassifier, self).__init__()\n",
        "#         self.gpt2_for_classification = gpt2_model\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#         total_hidden_size = gpt2_model.config.hidden_size * gpt2_model.config.num_attention_heads\n",
        "#         # Add batch normalization after the hidden layer\n",
        "#         self.batch_norm = nn.BatchNorm1d(total_hidden_size)\n",
        "\n",
        "#         # Add dropout layer\n",
        "#         self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "#         # Linear layer for classification\n",
        "#         self.classification_head = nn.Linear(gpt2_model.config.hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         outputs = self.gpt2_for_classification(input_ids, attention_mask=attention_mask)\n",
        "#         if hasattr(outputs, \"past_key_values\"):\n",
        "#             last_hidden_state = outputs.past_key_values[0][-1]\n",
        "#         else:\n",
        "#             last_hidden_state = outputs.last_hidden_states\n",
        "\n",
        "#         batch_size, num_heads, sequence_length, hidden_size_per_head = last_hidden_state.size()\n",
        "#         last_hidden_state_reshaped = last_hidden_state.reshape(batch_size * num_heads * sequence_length, hidden_size_per_head)\n",
        "#\n",
        "          # Apply batch normalization\n",
        "#         normalized_hidden_state = self.batch_norm(last_hidden_state_reshaped.permute(1, 0).contiguous().view(1, -1))\n",
        "\n",
        "#         # Apply dropout\n",
        "#         normalized_hidden_state = self.dropout(normalized_hidden_state)\n",
        "\n",
        "#         # Get logits for classification\n",
        "#         logits = self.classification_head(normalized_hidden_state[:, 0, :])\n",
        "\n",
        "#         return logits\n",
        "\n",
        "# num_classes = len(set(mood))\n",
        "# config = GPT2Config.from_pretrained(model_name, num_labels=num_classes, pad_token_id=50256)\n",
        "# gpt2_model = GPT2ForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "# model = GPT2MoodClassifier(gpt2_model, num_classes=num_classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "pGnVwKzYhnXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_-euMm2078G"
      },
      "outputs": [],
      "source": [
        "# Loss Function, Optimizer and Dataloader\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "weight_decay = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-6, weight_decay = weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size = 2, shuffle = True)\n",
        "val_dataloader = DataLoader(val_data, batch_size = 2, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otKXwOh3078G",
        "outputId": "f4f6849e-f5b4-46c2-f751-d8876fed098c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2MoodClassifier(\n",
              "  (gpt2_for_classification): GPT2ForSequenceClassification(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (score): Linear(in_features=768, out_features=4, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAn1RqpguddU"
      },
      "source": [
        "# **Original training and validation loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwTpi3JX078G"
      },
      "outputs": [],
      "source": [
        "# T_loss = []\n",
        "# V_loss = []\n",
        "# T_acc = []\n",
        "# V_acc = []\n",
        "# T_f1 = []\n",
        "# V_f1 = []\n",
        "\n",
        "# for epoch in range(5):\n",
        "\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     train_pred = []\n",
        "#     train_true = []\n",
        "\n",
        "#     for batch in tqdm(train_dataloader):\n",
        "#         input_ids = batch[0].to(device)\n",
        "#         labels = batch[1].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         logits = model(input_ids)\n",
        "#         loss_value = loss(logits, labels)\n",
        "\n",
        "#         prob = nn.functional.softmax(logits, dim = -1)\n",
        "#         pred = torch.argmax(prob, dim = -1)\n",
        "\n",
        "#         loss_value.backward()\n",
        "\n",
        "#         train_loss += loss_value.item()\n",
        "#         train_pred.extend(pred.cpu().numpy())\n",
        "#         train_true.extend(labels.cpu().numpy())\n",
        "\n",
        "#         optimizer.step()\n",
        "\n",
        "#     scheduler.step()\n",
        "\n",
        "#     T_acc.append(accuracy_score(train_true, train_pred))\n",
        "#     T_f1.append(f1_score(train_true, train_pred, average = 'weighted'))\n",
        "#     T_loss.append(train_loss/len(train_dataloader))\n",
        "\n",
        "#     print(\"Epoch: {}, Training Loss: {:.4f}, Accuracy: {:.4f}, F1 Score: {:.4f}\"\\\n",
        "#           .format(epoch, train_loss/len(train_dataloader), \\\n",
        "#             accuracy_score(train_true, train_pred), f1_score(train_true, train_pred, average = 'weighted')))\n",
        "\n",
        "\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     val_pred = []\n",
        "#     val_true = []\n",
        "\n",
        "#     for batch in val_dataloader:\n",
        "#         input_ids = batch[0].to(device)\n",
        "#         labels = batch[1].to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             logits = model(input_ids)\n",
        "#             prob = nn.functional.softmax(logits, dim = -1)\n",
        "#             pred = torch.argmax(prob, dim = -1)\n",
        "\n",
        "#             loss_value = loss(logits, labels)\n",
        "#             val_loss += loss_value.item()\n",
        "#             val_pred.extend(pred.cpu().numpy())\n",
        "#             val_true.extend(labels.cpu().numpy())\n",
        "\n",
        "#     V_acc.append(accuracy_score(val_true, val_pred))\n",
        "#     V_f1.append(f1_score(val_true, val_pred, average = 'weighted'))\n",
        "#     V_loss.append(val_loss/len(val_dataloader))\n",
        "\n",
        "#     print(\"Epoch: {}, Validation Loss: {:.4f}, Accuracy: {:.4f}, F1 Score: {:.4f}\"\\\n",
        "#           .format(epoch, val_loss/len(val_dataloader), accuracy_score(val_true, val_pred), \\\n",
        "#             f1_score(val_true, val_pred, average = 'weighted')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2xSBW6uh4Z"
      },
      "source": [
        "# **Optimization using gradient accumulation and mixed precision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK6UJCxoqnuO"
      },
      "outputs": [],
      "source": [
        "T_loss = []\n",
        "V_loss = []\n",
        "T_acc = []\n",
        "V_acc = []\n",
        "T_f1 = []\n",
        "V_f1 = []\n",
        "\n",
        "accumulation_steps = 2\n",
        "checkpoint = 5\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(30):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_pred = []\n",
        "    train_true = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "          logits = model(input_ids,attention_mask)\n",
        "          loss_value = loss(logits, labels)\n",
        "\n",
        "        scaler.scale(loss_value).backward()\n",
        "\n",
        "        if (step+ 1)% accumulation_steps ==0 :\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "        prob = nn.functional.softmax(logits, dim = -1)\n",
        "        pred = torch.argmax(prob, dim = -1)\n",
        "\n",
        "\n",
        "        train_loss += loss_value.item()\n",
        "        train_pred.extend(pred.cpu().numpy())\n",
        "        train_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch%checkpoint == 0:\n",
        "      torch.save({\n",
        "          'epoch' : epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss' : loss_value,\n",
        "      }, checkpoint_path)\n",
        "\n",
        "    T_acc.append(accuracy_score(train_true, train_pred))\n",
        "    T_f1.append(f1_score(train_true, train_pred, average = 'weighted'))\n",
        "    T_loss.append(train_loss/len(train_dataloader))\n",
        "\n",
        "    print(\"Epoch: {}, Training Loss: {:.4f}, Accuracy: {:.4f}, F1 Score: {:.4f}\"\\\n",
        "          .format(epoch, train_loss/len(train_dataloader), \\\n",
        "            accuracy_score(train_true, train_pred), f1_score(train_true, train_pred, average = 'weighted')))\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_pred = []\n",
        "    val_true = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            prob = nn.functional.softmax(logits, dim = -1)\n",
        "            pred = torch.argmax(prob, dim = -1)\n",
        "\n",
        "            loss_value = loss(logits, labels)\n",
        "            val_loss += loss_value.item()\n",
        "            val_pred.extend(pred.cpu().numpy())\n",
        "            val_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    V_acc.append(accuracy_score(val_true, val_pred))\n",
        "    V_f1.append(f1_score(val_true, val_pred, average = 'weighted'))\n",
        "    V_loss.append(val_loss/len(val_dataloader))\n",
        "\n",
        "    print(\"Epoch: {}, Validation Loss: {:.4f}, Accuracy: {:.4f}, F1 Score: {:.4f}\"\\\n",
        "          .format(epoch, val_loss/len(val_dataloader), accuracy_score(val_true, val_pred), \\\n",
        "            f1_score(val_true, val_pred, average = 'weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS59uyap078H"
      },
      "outputs": [],
      "source": [
        "#plotting all the metrics\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(T_loss, label='Training Loss')\n",
        "plt.plot(V_loss, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(T_acc, label='Training Accuracy')\n",
        "plt.plot(V_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting F1 Score\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(T_f1, label='Training F1 Score')\n",
        "plt.plot(V_f1, label='Validation F1 Score')\n",
        "plt.title('F1 Score Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}